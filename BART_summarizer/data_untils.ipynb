{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    files = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            files.append(line.strip())\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11307\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/ml/cadencao/XSum/fairseq_files/val.source'\n",
    "target_path = '/home/ml/cadencao/XSum/fairseq_files/val.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11307\n"
     ]
    }
   ],
   "source": [
    "document_bpe_path = '/home/ml/cadencao/XSum/fairseq_files/val.bpe.source'\n",
    "target_bpe_path = '/home/ml/cadencao/XSum/fairseq_files/val.bpe.target'\n",
    "xsum_bpe_source = read_lines(document_bpe_path)\n",
    "xsum_bpe_target = read_lines(target_bpe_path)\n",
    "print(len(xsum_bpe_source))\n",
    "assert len(xsum_bpe_source) == len(xsum_bpe_target) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.bart import BARTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/ml/cadencao/Downloads/BART_models/bart.large',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/ml/cadencao/Downloads/BART_models/bart.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- activate evaluation mode\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- activate evaluation mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.tasks.denoising.DenoisingTask at 0x7f7ba1594ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Input to BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.tasks.translation import TranslationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atletico Madrid goalkeeper David de Gea has said he would not feel the pressure of a high transfer fee if he were to join Manchester United.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_func(torch.LongTensor([ 3750, 12533,  3622,  7551,   871,   263,  4177,   102,    34,    26,\n",
    "           37,    74,    45,   619,     5,  1164,     9,    10,   239,  2937,\n",
    "         4029,   114,    37,    58,     7,  1962,  2361,   315,     4,     2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Striker Robert Vittek is the headline absentee from the provisional 27-man squad Slovakia coach Jan Kozak has named for Euro 2016.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_target[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  3750, 12533,  3622,  7551,   871,   263,  4177,   102,    34,\n",
       "           26,    37,    74,    45,   619,     5,  1164,     9,    10,   239,\n",
       "         2937,  4029,   114,    37,    58,     7,  1962,  2361,   315,     4,\n",
       "            2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_func('Atletico Madrid goalkeeper David de Gea has said he would not feel the pressure of a high transfer fee if he were to join Manchester United.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ml/cadencao/XSum/test_files/xsum-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[source] dictionary: 50264 types\n",
      "[target] dictionary: 50264 types\n"
     ]
    }
   ],
   "source": [
    "src_dict = TranslationTask.load_dictionary(os.path.join(path, 'dict.{}.txt'.format('source')))\n",
    "tgt_dict = TranslationTask.load_dictionary(os.path.join(path, 'dict.{}.txt'.format('target')))\n",
    "assert src_dict.pad() == tgt_dict.pad()\n",
    "assert src_dict.eos() == tgt_dict.eos()\n",
    "assert src_dict.unk() == tgt_dict.unk()\n",
    "print('[{}] dictionary: {} types'.format('source', len(src_dict)))\n",
    "print('[{}] dictionary: {} types'.format('target', len(tgt_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.dictionary.Dictionary at 0x7f7ba03e7790>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_to_ids(fairseq_dict, sentence, addl_sentence=None, max_positions=1024, no_bos=True):\n",
    "    \"\"\"Convert bpe ids to model input ids.\n",
    "\n",
    "    Args:\n",
    "        fairseq_dict (fairseq.data.dictionary.Dictionary): fairseq dictionary.\n",
    "        sentence (str): bpe encoded sentence.\n",
    "        addl_sentence (str): bpe encoded sentence.\n",
    "        max_positions (int): max sentence length.\n",
    "        no_bos (bool): whether append bos token.\n",
    "\n",
    "    \"\"\"\n",
    "    extra_tokens = 2\n",
    "    bos, eos = '<s> ', ' </s>'\n",
    "\n",
    "    if no_bos:\n",
    "        bos = ''\n",
    "        extra_tokens = 1\n",
    "\n",
    "    if addl_sentence:\n",
    "        tokens = sentence + eos + ' ' + addl_sentence\n",
    "    else:\n",
    "        tokens = sentence\n",
    "\n",
    "    if len(tokens.split(' ')) > max_positions - extra_tokens:\n",
    "        tokens = ' '.join(tokens.split(' ')[:max_positions - extra_tokens])\n",
    "    bpe_sentence = bos + tokens + eos\n",
    "\n",
    "    tokens = fairseq_dict.encode_line(bpe_sentence, append_eos=False)\n",
    "    return tokens.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12510 1966 3701 4881 4409 423 587 1043 6717 286 34759 262 20858 422 262 12983 286 734 12353 19105 257 3249 546 1693 6630 13'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_bpe_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15622,   320,  1754,  1470,  1321,    33,    57,   303,  2181,     9,\n",
      "        28304,     5, 15331,    31,     5,  7314,     9,    80,  4585,  9886,\n",
      "           10,   529,    59,   633,  2599,     4,     2])\n"
     ]
    }
   ],
   "source": [
    "ids = bpe_to_ids(src_dict, xsum_bpe_target[0])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three former Air France employees have been found guilty of ripping the shirts from the backs of two executives fleeing a meeting about job cuts.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_func(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.language_pair_dataset import collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(3):\n",
    "    samples.append({\n",
    "        'id': i,\n",
    "        'source': bpe_to_ids(src_dict, xsum_bpe_source[i]),\n",
    "        'target': bpe_to_ids(src_dict, xsum_bpe_target[i])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx, eos_idx = 1, 2\n",
    "batch = collate(samples, pad_idx, eos_idx, left_pad_source=True, left_pad_target=False, input_feeding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([2, 0, 1]),\n",
       " 'nsentences': 3,\n",
       " 'ntokens': 65,\n",
       " 'net_input': {'src_tokens': tensor([[ 100,   33,   57,  ...,  502,    4,    2],\n",
       "          [   1,    1,    1,  ..., 1427,    4,    2],\n",
       "          [   1,    1,    1,  ...,  443,    4,    2]]),\n",
       "  'src_lengths': tensor([668, 376, 165]),\n",
       "  'prev_output_tokens': tensor([[    2, 29042,  1252,    11,  5295,    28,   357,   160,    11,    50,\n",
       "              66,     9,     5,   796,  1332,   116,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1],\n",
       "          [    2, 15622,   320,  1754,  1470,  1321,    33,    57,   303,  2181,\n",
       "               9, 28304,     5, 15331,    31,     5,  7314,     9,    80,  4585,\n",
       "            9886,    10,   529,    59,   633,  2599,     4],\n",
       "          [    2,  9497,    33,   703, 14363,  3156,    25,   233,     9,  4941,\n",
       "              88,    41,  2080,   751,    41,  1586,  7450,  2681,  2003,    94,\n",
       "              76,     4,     1,     1,     1,     1,     1]])},\n",
       " 'target': tensor([[29042,  1252,    11,  5295,    28,   357,   160,    11,    50,    66,\n",
       "              9,     5,   796,  1332,   116,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1],\n",
       "         [15622,   320,  1754,  1470,  1321,    33,    57,   303,  2181,     9,\n",
       "          28304,     5, 15331,    31,     5,  7314,     9,    80,  4585,  9886,\n",
       "             10,   529,    59,   633,  2599,     4,     2],\n",
       "         [ 9497,    33,   703, 14363,  3156,    25,   233,     9,  4941,    88,\n",
       "             41,  2080,   751,    41,  1586,  7450,  2681,  2003,    94,    76,\n",
       "              4,     2,     1,     1,     1,     1,     1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, fairseq_dict, source_path, target_path, max_positions=1024, no_bos=True, pad_idx=1, eos_idx=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fairseq_dict (fairseq.data.dictionary.Dictionary): fairseq dictionary.\n",
    "            source_path (str): path to bpe encoded source.\n",
    "            target_path (str): path to bpe encoded target.\n",
    "            max_positions (int): max sentence length.\n",
    "            no_bos (bool): whether append bos token.\n",
    "\n",
    "        \"\"\"\n",
    "        self.fairseq_dict = fairseq_dict\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "\n",
    "        self.max_positions = max_positions\n",
    "        self.no_bos = no_bos\n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "\n",
    "        source = DataLoader.read_lines(source_path)\n",
    "        target = DataLoader.read_lines(target_path)\n",
    "        assert len(source) == len(target), \"Source and target size do NOT match!\"\n",
    "\n",
    "        self.data = self.build_sample(source, target)\n",
    "        del source, target\n",
    "\n",
    "    def build_sample(self, source, target):\n",
    "        data = []\n",
    "        for i, (s, t) in enumerate(zip(source, target)):\n",
    "            data.append({\n",
    "                'id': i,\n",
    "                'source': self.bpe_to_ids(s),\n",
    "                'target': self.bpe_to_ids(t)\n",
    "            })\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def read_lines(file_path):\n",
    "        files = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                files.append(line.strip())\n",
    "        return files\n",
    "\n",
    "    def batch_iter(self, batch_size):\n",
    "        \"\"\"Create a batch of data.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for d in self.data:\n",
    "            if len(samples) == batch_size:\n",
    "                yield collate(samples, self.pad_idx, self.eos_idx, \n",
    "                              left_pad_source=True,\n",
    "                              left_pad_target=False,\n",
    "                              input_feeding=True)\n",
    "                samples = []\n",
    "\n",
    "            samples += [d]\n",
    "\n",
    "        if len(samples) != 0:\n",
    "            yield collate(samples, self.pad_idx, self.eos_idx,\n",
    "                          left_pad_source=True,\n",
    "                          left_pad_target=False,\n",
    "                          input_feeding=True)\n",
    "\n",
    "    def bpe_to_ids(self, sentence, addl_sentence=None):\n",
    "        \"\"\"Convert bpe ids to model input ids.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): bpe encoded sentence.\n",
    "            addl_sentence (str): bpe encoded sentence.\n",
    "\n",
    "        \"\"\"\n",
    "        extra_tokens = 2\n",
    "        bos, eos = '<s> ', ' </s>'\n",
    "\n",
    "        if self.no_bos:\n",
    "            bos = ''\n",
    "            extra_tokens = 1\n",
    "\n",
    "        if addl_sentence:\n",
    "            tokens = sentence + eos + ' ' + addl_sentence\n",
    "        else:\n",
    "            tokens = sentence\n",
    "\n",
    "        if len(tokens.split(' ')) > self.max_positions - extra_tokens:\n",
    "            tokens = ' '.join(tokens.split(' ')[:self.max_positions - extra_tokens])\n",
    "        bpe_sentence = bos + tokens + eos\n",
    "\n",
    "        tokens = self.fairseq_dict.encode_line(bpe_sentence, append_eos=False)\n",
    "        return tokens.long()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = DataLoader(src_dict, document_bpe_path, target_bpe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in val.batch_iter(24):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(val.batch_iter(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([ 8179,  6091,   624, 10775]),\n",
       " 'nsentences': 4,\n",
       " 'ntokens': 111,\n",
       " 'net_input': {'src_tokens': tensor([[28061,   917,    32,  ...,   296,     4,     2],\n",
       "          [    1,     1,     1,  ...,   158,    72,     2],\n",
       "          [    1,     1,     1,  ...,  1102,     4,     2],\n",
       "          [    1,     1,     1,  ...,  9243,     4,     2]]),\n",
       "  'src_lengths': tensor([363, 285, 273, 196]),\n",
       "  'prev_output_tokens': tensor([[    2,   133,   248, 11674,  2918,    34,  1695,     5,  6305,     9,\n",
       "             112,     6,   151,  2944,  5569,   518,   228,    76,    32,  1633,\n",
       "             396,    10,   200,   919,     9,   813,    15,   792,     4,     1,\n",
       "               1,     1],\n",
       "          [    2, 43623,   158,    34,  2296,  1449,    11,    10,    92,  1040,\n",
       "              14,  6045,     5, 18123,   852,  4758,    16, 40891,    30,     5,\n",
       "            2654,  1269,     8,    39,   284,     4,     1,     1,     1,     1,\n",
       "               1,     1],\n",
       "          [    2,   250,  3862,    12, 21831,   891,    56,    41,   515,  2650,\n",
       "             386,     7,    49,  3397,    77,    51,   303,  1235,  5930,    11,\n",
       "              10, 22637,  5619,     4,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1],\n",
       "          [    2, 40622,  3065,  1204,    34,  1507,     7, 31047,    68,  1497,\n",
       "             119,    36,  2537, 29254, 32230,   119,    43,    13, 10467,   804,\n",
       "            2329, 24038,    31,  1563, 25635,  2183,  2439,  2196,     7,   382,\n",
       "             916,     4]])},\n",
       " 'target': tensor([[  133,   248, 11674,  2918,    34,  1695,     5,  6305,     9,   112,\n",
       "              6,   151,  2944,  5569,   518,   228,    76,    32,  1633,   396,\n",
       "             10,   200,   919,     9,   813,    15,   792,     4,     2,     1,\n",
       "              1,     1],\n",
       "         [43623,   158,    34,  2296,  1449,    11,    10,    92,  1040,    14,\n",
       "           6045,     5, 18123,   852,  4758,    16, 40891,    30,     5,  2654,\n",
       "           1269,     8,    39,   284,     4,     2,     1,     1,     1,     1,\n",
       "              1,     1],\n",
       "         [  250,  3862,    12, 21831,   891,    56,    41,   515,  2650,   386,\n",
       "              7,    49,  3397,    77,    51,   303,  1235,  5930,    11,    10,\n",
       "          22637,  5619,     4,     2,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1],\n",
       "         [40622,  3065,  1204,    34,  1507,     7, 31047,    68,  1497,   119,\n",
       "             36,  2537, 29254, 32230,   119,    43,    13, 10467,   804,  2329,\n",
       "          24038,    31,  1563, 25635,  2183,  2439,  2196,     7,   382,   916,\n",
       "              4,     2]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.utils import move_to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = move_to_cuda(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output = bart.model(**test_batch['net_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 50265])\n"
     ]
    }
   ],
   "source": [
    "print(net_output[0].shape) # [batch_size, target_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attn', 'inner_states'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_output[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs = bart.model.get_normalized_probs(net_output, log_probs=True)\n",
    "lprobs = lprobs.view(-1, lprobs.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 50265])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = test_batch[\"target\"].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduce=True):\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "    if reduce:\n",
    "        nll_loss = nll_loss.sum()\n",
    "        smooth_loss = smooth_loss.sum()\n",
    "    eps_i = epsilon / lprobs.size(-1)\n",
    "    loss = (1. - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = 1\n",
    "eps = 0.1\n",
    "loss, nll_loss = label_smoothed_nll_loss(\n",
    "    lprobs, target, eps, ignore_index=padding_idx, reduce=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(996.7502, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # scheduler\n",
    "        self.warmup_updates = 500\n",
    "        self.end_learning_rate = 0.00\n",
    "        self.total_num_update = 20000\n",
    "        self.power = 1.0\n",
    "        self.force_anneal = None\n",
    "        \n",
    "        # optimizer\n",
    "        self.lr = [3e-5]\n",
    "        self.adam_betas = '(0.9, 0.999)'\n",
    "        self.adam_eps = 1e-8\n",
    "        self.weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.optim.adam import Adam, FairseqAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(\n",
    "            filter(\n",
    "                lambda p: p.requires_grad,\n",
    "                bart.model.parameters(),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = FairseqAdam(args, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.optim.lr_scheduler.polynomial_decay_schedule import PolynomialDecaySchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PolynomialDecaySchedule(args, adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.step_update(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam.backward(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norm = adam.clip_grad_norm(0.1, aggregate_norm_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
