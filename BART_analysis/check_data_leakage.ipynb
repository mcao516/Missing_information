{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proof-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from utils import read_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fluid-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = json.load(open('path_config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stylish-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = PATH['xsum_fariseq'] + '/test.source'\n",
    "target_path = PATH['xsum_fariseq'] + '/test.target'\n",
    "test_source = read_lines(document_path)\n",
    "test_target = read_lines(target_path)\n",
    "print(len(test_source))\n",
    "assert len(test_source) == len(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suitable-vacation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203575\n"
     ]
    }
   ],
   "source": [
    "document_path = PATH['xsum_fariseq'] + '/train.source'\n",
    "target_path = PATH['xsum_fariseq'] + '/train.target'\n",
    "train_source = read_lines(document_path)\n",
    "train_target = read_lines(target_path)\n",
    "print(len(train_source))\n",
    "assert len(train_source) == len(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-aerospace",
   "metadata": {},
   "source": [
    "#### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "insured-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_entities = json.load(open('right_entities.json'))\n",
    "left_entities = json.load(open('left_entities.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spectacular-exclusive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 74,\n",
       " 'end': 79,\n",
       " 'label': 1,\n",
       " 'type': 'PERSON',\n",
       " 'ent': 'David',\n",
       " 'prior': 0.00992584228515625,\n",
       " 'posterior': 0.94140625,\n",
       " 'id': 1513}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_entities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thermal-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Times Educational Supplement (TES) says measures to guard against grade inflation were not appropriate for this year\\'s English exams. Teaching unions have accused Ofqual of \"regulatory failure\" and say it is more evidence of flawed exam marking. But the regulator insists it applied its measures correctly. It comes as state and private school heads call for an independent inquiry into the problems. Last week Ofqual said it felt the way this year\\'s English GCSE exams were graded was fair, despite the  grade boundaries being moved significantly part-way through the year. Teachers have complained that pupils achieving exactly the same marks would have received different grades depending on what time of year they sat the exam. And many who were predicted a grade C, obtained a grade D in their summer exams. Ofqual found that June\\'s grade boundaries were correct, while January\\'s were \"too lenient\". A key document sent to exam boards, which is on the Ofqual website, sets out how it expects exam boards to use a process known as \"comparable outcomes\" to ensure that grade standards are maintained between GCSEs year-on-year. It sets out five conditions for when such processes should be used to alter the way papers are marked. According to the TES report, this year\\'s English GCSE and the circumstances around it failed to meet four of the conditions. The first of Ofqual\\'s five conditions was that the cohort - or pupils from that year- in a subject \"must be similar, in terms of ability, to those of previous years\". But last week\\'s report on English GCSEs noted that the \"attainment profile\" for 2012 dropped, as grammar and independent school pupils were replaced with extra, lower-achieving comprehensive pupils. The second condition was that the qualification must be \"fit for purpose\". On Monday, Education Secretary Michael Gove told Parliament that this year\\'s English GCSE was \"not fit for purpose\". The third condition was that the \"nature of the qualification\" must be the same. Ofqual\\'s report last week stated that \"these qualifications are different from previous English qualifications in a number of ways\". Finally, the regulator said in the letter that comparable outcomes must only be applied where \"previous grades were appropriate\". Ofqual\\'s report said that the English GCSE grades in January were \"too generous\". The only condition that appears to have been met is that teaching standards have remained largely consistent. The guidelines also state: \"Students should not be advantaged or disadvantaged simply because they were the first to sit a new set of examinations.\" The watchdog has interpreted its own conditions differently, and is insisting that the comparable outcomes approach that prevented grades from rising in June was correctly implemented. But Russell Hobby, general secretary of the NAHT heads\\' union, said: \"Ofqual doesn\\'t seem to have applied the concept of comparable outcomes properly. \"In this instance there is a regulatory failure. It is a big mess.\" And Brian Lightman, general secretary of the Association of School and College Leaders (ASCL) said this appeared to be \"another piece of evidence that procedures applied to this exam was not fit for purpose\". Meanwhile, Ofqual said: \"The comparable outcomes approach was used correctly for GCSE English this year and we stand by this decision. \"Comparable outcomes are not intended to produce identical results year on year. \"What\\'s important is that outcomes can be compared when taking into consideration a range of factors which may affect the results. More details on the use of this approach can be found in our report.\" But Dr Christopher Ray, chairman of the Head Masters\\' and Headmistresses\\' Conference (HMC), said the issue was evidence of a deeper problem with Ofqual. \"The verdict of Ofqual\\'s initial report and the reasoning to support it fall well short of answering the questions raised in the minds of schools and pupils.\" Shadow education secretary Stephen Twigg said: \"It is clear that pupils, parents and education professionals, across the spectrum of schools, feel that pupils have been done a disservice. \"Whilst the Education Secretary Michael Gove says he is \\'saddened\\' by the injustice that has been served to thousands of pupils, he is showing how out-of-touch he is with pupil opinion by refusing to take action. Labour supports calls for an independent inquiry to get to the bottom of this mess.\" The Commons Education Committee is set to start an inquiry into the GCSE grading problems next week.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_source[922]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-psychiatry",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "northern-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "signed-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_source = []\n",
    "for ent in left_entities:\n",
    "    left_source.append(test_source[ent['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coordinated-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_source = []\n",
    "for ent in right_entities:\n",
    "    right_source.append(test_source[ent['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oriental-problem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(left_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "national-multiple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(right_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "minor-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203611\n"
     ]
    }
   ],
   "source": [
    "concat_corpus = []\n",
    "concat_corpus.extend(left_source)\n",
    "concat_corpus.extend(right_source)\n",
    "concat_corpus.extend(train_source)\n",
    "print(len(concat_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indirect-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_tfidf = TfidfVectorizer().fit_transform(concat_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fuzzy-scout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203611, 265359)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-bryan",
   "metadata": {},
   "source": [
    "#### Calculate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spare-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_vector = train_source_tfidf[0: len(left_source), :]\n",
    "right_vector = train_source_tfidf[len(left_source): len(left_source) + len(right_source), :]\n",
    "source_vector = train_source_tfidf[len(left_source) + len(right_source):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lesbian-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 265359)\n",
      "(18, 265359)\n",
      "(203575, 265359)\n"
     ]
    }
   ],
   "source": [
    "print(left_vector.shape)\n",
    "print(right_vector.shape)\n",
    "print(source_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "precious-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_similarity = source_vector * left_vector.T\n",
    "right_similarity = source_vector * right_vector.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-cabinet",
   "metadata": {},
   "source": [
    "#### Retrival Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "completed-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lovely-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203575, 18)\n",
      "(203575, 18)\n"
     ]
    }
   ],
   "source": [
    "print(left_similarity.shape)\n",
    "print(right_similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "legitimate-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity(similarity_matrix, entities, k=3):\n",
    "    \"\"\"similarity_matrix: (203575, 18)\"\"\"\n",
    "    similarity_matrix = torch.tensor(similarity_matrix.toarray())\n",
    "    topk = torch.topk(similarity_matrix, k, dim=0)[1]  # [k, entity_number]: [10, 18]\n",
    "    \n",
    "    related_documents, related_summaries = [], []\n",
    "    for i in range(topk.shape[1]):\n",
    "        related_documents.append([])\n",
    "        related_summaries.append([])\n",
    "        for k in range(topk.shape[0]):\n",
    "            related_documents[i].append(train_source[topk[k][i]])\n",
    "            related_summaries[i].append(train_target[topk[k][i]])\n",
    "    \n",
    "    doc_counts, sum_counts = [], []\n",
    "    for i in range(topk.shape[1]):\n",
    "        source = ' '.join(related_documents[i])\n",
    "        target = ' '.join(related_summaries[i])\n",
    "        doc_counts.append(source.count(entities[i]['ent']))\n",
    "        sum_counts.append(target.count(entities[i]['ent']))\n",
    "    return doc_counts, sum_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "pharmaceutical-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_doc_counts, left_sum_counts = count_entity(left_similarity, left_entities, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "illegal-debate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 7, 13, 1, 0, 3, 8, 5, 7, 4, 0, 100, 12, 4, 0, 13, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_doc_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "authorized-harris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 2, 3, 7, 2, 2, 4, 8, 3, 9, 0, 6, 4, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_sum_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fatty-covering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.555555555555555"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(left_doc_counts) / len(left_doc_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "double-master",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6666666666666665"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(left_sum_counts) / len(left_sum_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "contemporary-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_doc_counts, right_sum_counts = count_entity(right_similarity, right_entities, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "known-tumor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.333333333333333"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(right_doc_counts) / len(right_doc_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "previous-married",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1666666666666665"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(right_sum_counts) / len(right_sum_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "several-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 6,\n",
       " 'end': 16,\n",
       " 'label': 1,\n",
       " 'type': 'ORG',\n",
       " 'ent': 'Centurions',\n",
       " 'prior': 0.004512786865234375,\n",
       " 'posterior': 0.85888671875,\n",
       " 'id': 6824}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "european-intention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 30-year-old moved to Leigh in November on a two-year contract after his release by Salford following a \"disciplinary procedure\". Chase won the 2011 Man of Steel while at Castleford, also earning his 11 England caps during a four-year stay. The New Zealand-born stand-off scored two tries in his five appearances for the Centurions this season, with his last appearance on 1 May. \"He came to Leigh and wanted to fall back in love with the game,\" owner Derek Beaumont told BBC Radio Manchester. \"He came from a difficult situation from Salford and it\\'s proven hard to do that. \"He\\'s had a couple of things that have been made available to him as an opportunity and there are a couple of options he can explore outside of the game. \"He came and spoke to me and felt it was in his best interests if he was given that opportunity and I\\'ve accepted that.\" Leigh signed Australian former London Broncos half-back Josh Drinkwater this week from West Tigers until the end of the season.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source[17646]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-ireland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
