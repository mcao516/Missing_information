{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fairseq.models.bart import BARTModel\n",
    "from utils import read_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bart = BARTModel.from_pretrained('/home/mcao610/scratch/BART_models/xsum_cmlm_ent',\n",
    "                                           checkpoint_file='checkpoint_best.pt',\n",
    "                                           data_name_or_path='/home/mcao610/scratch/summarization/XSum/fairseq_files/xsum-bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fine-tuned bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "finetuned_bart.cuda()\n",
    "finetuned_bart.eval()\n",
    "finetuned_bart.half()\n",
    "print('- fine-tuned bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/mcao610/scratch/BART_models/bart.large',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/mcao610/scratch/BART_models/bart.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/mcao610/scratch/summarization/XSum/fairseq_files/test.source'\n",
    "target_path = '/home/mcao610/scratch/summarization/XSum/fairseq_files/test.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens\n",
    "from utils import get_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(src_input, verbose=False):\n",
    "    src_inputs = [src_input]  # list of input string\n",
    "    src_tokens = collate_tokens([encode_func(i) for i in src_inputs], pad_idx=1, left_pad=True)\n",
    "    src_tokens = src_tokens.cuda()\n",
    "    src_lengths = torch.sum(src_tokens != 1, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('- src tokens: {};\\n- src lengths: {}'.format(src_tokens.shape, src_lengths.shape))\n",
    "    return src_tokens, src_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_mask(input_sentence):\n",
    "    bpe_code = bart.bpe.encode(input_sentence)  # <mask>: 1279 27932 29\n",
    "    input_ids = bart.task.source_dictionary.encode_line('<s> ' + bpe_code.replace('1279 27932 29', '<mask>'), \n",
    "                                                        append_eos=True).long()\n",
    "    input_ids = input_ids.unsqueeze(0).cuda()\n",
    "    src_lengths = torch.sum(input_ids != 1, dim=1)\n",
    "    return input_ids, src_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(decoder, encoder_out, batch_size=1, tgt_tokens=None, min_decode_step=1, max_decode_step=100, pad_id=1, eos_id=2, verbose=True):\n",
    "    init_input = torch.tensor([[2, 0]] * batch_size, dtype=torch.long).cuda()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    token_probs, tokens = [], []\n",
    "\n",
    "    for step in range(max_decode_step):\n",
    "        decoder_outputs = decoder(init_input, encoder_out, features_only=False)\n",
    "        logits = decoder_outputs[0][:, -1, :]  # [batch_size, vocab]\n",
    "        \n",
    "        if step + 1 < min_decode_step:\n",
    "            logits[:, eos_id] = -math.inf\n",
    "        logits[:, pad_id], logits[:, 0] = -math.inf, -math.inf  # never select pad, start token\n",
    "\n",
    "        probs = softmax(logits)\n",
    "        assert logits.shape == probs.shape\n",
    "        attn = decoder_outputs[1]['attn'][0]  # [batch_size, prev_token_len, src_token_len]\n",
    "        assert logits.dim() == 2 and attn.dim() == 3\n",
    "\n",
    "        if tgt_tokens is not None:\n",
    "            selected_token = tgt_tokens[step].unsqueeze(0)\n",
    "        else:\n",
    "            value, indices = torch.topk(probs, 5, dim=1)\n",
    "            selected_token = indices[:, 0]\n",
    "\n",
    "        init_input = torch.cat([init_input, selected_token.unsqueeze(1)], dim=-1)\n",
    "        token, prob = decode_func(selected_token), probs.squeeze()[selected_token.item()].item()\n",
    "        \n",
    "        if selected_token.item() == eos_id:\n",
    "            break\n",
    "        elif verbose:\n",
    "            print(\"- {:02d}: {} ({:.2f})\".format(step, token, prob), end='\\n')\n",
    "\n",
    "        token_probs.append(prob)\n",
    "        tokens.append(token)\n",
    "\n",
    "    return init_input, tokens, token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmlm_probability(bart_model, masked_sentence, entity, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input,\n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=bart_model.encode(entity)[1:].cuda(),\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == entity\n",
    "    assert ''.join(masked_tokens) == entity\n",
    "    \n",
    "    prob = 1.0\n",
    "    for i in range(3, len(masked_token_probs)):\n",
    "        prob *= masked_token_probs[i]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_probability(bart_model, sentence, masked_sentence, position, entity, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input,\n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=bart_model.encode(sentence)[1:].cuda(),\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == sentence, '{}; {}'.format(decode_func(masked_output_ids[0]), sentence)\n",
    "\n",
    "    return get_probability(position, masked_tokens, masked_token_probs, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmlm_generate(bart_model, masked_sentence, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input, \n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=None,\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    \n",
    "    return decode_func(masked_output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_generate(bart_model, masked_sentence):\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input, \n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=None,\n",
    "                                       verbose=False)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    \n",
    "    return decode_func(masked_output_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 9444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\n",
      "[{'start': 75, 'end': 82, 'label': 'DATE'}]\n",
      "\n",
      "Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in <mask> in a bid to save the lives of babies born with the condition.\n",
      "- prior: Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in the UK and a charity in a bid to save the lives of babies born with the condition.\n",
      "Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in ### in a bid to save the lives of babies born with the condition.\n",
      "- posterior: <s> London\n",
      "- entity: Cardiff\n",
      "- prior: 0.0011692047119140625\n",
      "- posterior: 0.061309814453125\n",
      "- ratio: 0.061 / 0.001 = 51.993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = xsum_source[INDEX]\n",
    "target = \"Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\"\n",
    "print(target)\n",
    "\n",
    "ent_parts = nlp(target).to_json()['ents']\n",
    "print(ent_parts, end='\\n\\n')\n",
    "\n",
    "for e in ent_parts:\n",
    "    entity = target[e['start']: e['end']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "        prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "        print(target[0: e['start']] + '<mask>' + target[e['end']:])\n",
    "        print('- prior: {}'.format(prior_generate(bart, masked_hypothesis)))\n",
    "\n",
    "        masked_hypothesis = target[0: e['start']] + '###' + target[e['end']:]\n",
    "        masked_hypothesis = '<s> ' + masked_hypothesis + ' <\\s> ' + source\n",
    "        posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                         masked_hypothesis,\n",
    "                                         '<s> ' + entity, \n",
    "                                         verbose=False)\n",
    "        print(target[0: e['start']] + '###' + target[e['end']:])\n",
    "        print('- posterior: {}'.format(cmlm_generate(finetuned_bart, masked_hypothesis, verbose=False)))\n",
    "\n",
    "        print('- entity: {}'.format(entity))\n",
    "        print('- prior: {}'.format(prior))\n",
    "        print('- posterior: {}'.format(posterior))\n",
    "        print('- ratio: {:.3f} / {:.3f} = {:.3f}'.format(posterior, prior, posterior / (prior + 1e-5)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open('annotated.json', 'r'))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10943,\n",
       " 'pred': \"A powerful cyclone has killed at least 11 people and injured more than 100 in Vanuatu, the Pacific nation's president has said.\",\n",
       " 'ents': [{'start': 30,\n",
       "   'end': 41,\n",
       "   'label': 2,\n",
       "   'type': 'CARDINAL',\n",
       "   'ent': 'at least 11'},\n",
       "  {'start': 61,\n",
       "   'end': 74,\n",
       "   'label': 2,\n",
       "   'type': 'CARDINAL',\n",
       "   'ent': 'more than 100'},\n",
       "  {'start': 78, 'end': 85, 'label': 0, 'type': 'GPE', 'ent': 'Vanuatu'},\n",
       "  {'start': 91, 'end': 98, 'label': 1, 'type': 'LOC', 'ent': 'Pacific'}],\n",
       " 'hallucinations': ['killed at least 11 people and injured more than 100',\n",
       "  \"the Pacific nation's president has said.\"]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hallucination ents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e6b22eb11b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#             print(cmlm_generate(finetuned_bart, masked_hypothesis, verbose=False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hallucination ents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correctness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INDEX: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hallucination ents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hallucination ents'"
     ]
    }
   ],
   "source": [
    "prior_posterior = []\n",
    "\n",
    "for INDEX in tqdm(range(len(data))):\n",
    "    source = xsum_source[data[INDEX]['id']]\n",
    "    \n",
    "    for i, e in enumerate(data[INDEX]['ents']):\n",
    "        target = data[INDEX]['pred']\n",
    "        entity = target[e['start']: e['end']]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "            prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "\n",
    "            masked_hypothesis = target[0: e['start']] + '###' + target[e['end']:]\n",
    "            masked_hypothesis = '<s> ' + masked_hypothesis + ' <\\s> ' + source\n",
    "            posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                             masked_hypothesis,\n",
    "                                             '<s> ' + entity, \n",
    "                                             verbose=False)\n",
    "#             print(target[0: e['start']] + '###' + target[e['end']:])\n",
    "#             print(cmlm_generate(finetuned_bart, masked_hypothesis, verbose=False))\n",
    "\n",
    "            assert len(data[INDEX]['hallucination ents']) == len(data[INDEX]['correctness']), 'INDEX: {}'.format(INDEX)\n",
    "            if i not in data[INDEX]['hallucination ents']:\n",
    "                label = 0\n",
    "            else:\n",
    "                if data[INDEX]['correctness'][data[INDEX]['hallucination ents'].index(i)]:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 2\n",
    "                    assert not data[INDEX]['correctness'][data[INDEX]['hallucination ents'].index(i)]\n",
    "\n",
    "            prior_posterior.append({'id': data[INDEX]['id'], \n",
    "                                    'prior': prior, \n",
    "                                    'posterior': posterior, \n",
    "                                    'entity': entity, \n",
    "                                    'entity pos': e, \n",
    "                                    'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prior_posterior))\n",
    "print(prior_posterior[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('prior_posterior.json', 'w') as fout:\n",
    "#     json.dump(prior_posterior , fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior_posterior = json.load(open('prior_posterior.json', 'r'))\n",
    "# print(len(prior_posterior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20.0, 10.0))\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
    "\n",
    "no_hallucinated = [(p['prior'], p['posterior']) for p in prior_posterior if p['label'] == 0]\n",
    "hallucinated_true = [(p['prior'], p['posterior']) for p in prior_posterior if p['label'] == 1]\n",
    "hallucinated_false = [(p['prior'], p['posterior']) for p in prior_posterior if p['label'] == 2]\n",
    "\n",
    "# ax.scatter([i[0] for i in no_hallucinated], \n",
    "#            [i[1] for i in no_hallucinated], c='tab:blue', s=[i[1]*100 + 40 for i in no_hallucinated], label='Non-hallucination', alpha=0.7)\n",
    "\n",
    "ax.scatter([i[0] for i in hallucinated_true], \n",
    "           [i[1] for i in hallucinated_true], c='tab:green', s=[i[1]*100 + 40 for i in hallucinated_true], label='Hallucination True', alpha=0.65)\n",
    "ax.scatter([i[0] for i in hallucinated_false], \n",
    "           [i[1] for i in hallucinated_false], c='tab:orange', s=[i[1]*100 + 40 for i in hallucinated_false], label='Hallucination False', alpha=0.6)\n",
    "\n",
    "ax.scatter([1.0], [1.0], c='tab:gray', s=10)\n",
    "\n",
    "ax.set_xlabel('Prior Probability')\n",
    "ax.set_ylabel('Posterior Probability')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.savefig('foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prior_posterior:\n",
    "    if p['label'] == 2 and p['posterior'] > 0.5:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_posterior_dict = {}\n",
    "\n",
    "for p in prior_posterior:\n",
    "    if p['id'] not in prior_posterior_dict:\n",
    "        prior_posterior_dict[p['id']] = []\n",
    "    prior_posterior_dict[p['id']].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_posterior[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = []\n",
    "ent_pred_label = []\n",
    "prob_pred_label = []\n",
    "\n",
    "for p in prior_posterior:\n",
    "    source = xsum_source[p['id']]\n",
    "    \n",
    "    if p['entity'].lower() in source.lower():\n",
    "        ent_pred_label.append(0)\n",
    "    else:\n",
    "        ent_pred_label.append(1)\n",
    "        \n",
    "    if p['label'] == 0 or p['label'] == 1:\n",
    "        true_label.append(0)\n",
    "    else:\n",
    "        true_label.append(1)\n",
    "\n",
    "    if p['posterior'] > 0.1 or p['prior'] > 0.2:\n",
    "        prob_pred_label.append(0)\n",
    "    else:\n",
    "        prob_pred_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_label, ent_pred_label, target_names=['Non-hallucination', 'Hallucination']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_label, prob_pred_label, target_names=['Non-hallucination', 'Hallucination']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ents = 0\n",
    "\n",
    "for d in data:\n",
    "    total_ents += len(d['hallucination ents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "89 / 326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "mu, sigma = 100, 15\n",
    "x = mu + sigma * np.random.randn(10000)\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 100, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.xlim(40, 160)\n",
    "plt.ylim(0, 0.03)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = xsum_source[8770]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
