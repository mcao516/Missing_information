{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fairseq.models.bart import BARTModel\n",
    "from utils import read_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bart = BARTModel.from_pretrained('/home/ml/cadencao/fairseq/checkpoints/xsum_cmlm_bos',\n",
    "                                           checkpoint_file='checkpoint_best.pt',\n",
    "                                           data_name_or_path='/home/ml/cadencao/XSum/fairseq_files/xsum-bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fine-tuned bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "finetuned_bart.cuda()\n",
    "finetuned_bart.eval()\n",
    "finetuned_bart.half()\n",
    "print('- fine-tuned bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/ml/cadencao/Downloads/BART_models/bart.large',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/ml/cadencao/Downloads/BART_models/bart.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/ml/cadencao/XSum/fairseq_files/test.source'\n",
    "target_path = '/home/ml/cadencao/XSum/fairseq_files/test.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(src_input, verbose=False):\n",
    "    src_inputs = [src_input]  # list of input string\n",
    "    src_tokens = collate_tokens([encode_func(i) for i in src_inputs], pad_idx=1, left_pad=True)\n",
    "    src_tokens = src_tokens.cuda()\n",
    "    src_lengths = torch.sum(src_tokens != 1, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('- src tokens: {};\\n- src lengths: {}'.format(src_tokens.shape, src_lengths.shape))\n",
    "    return src_tokens, src_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_mask(input_sentence):\n",
    "    bpe_code = bart.bpe.encode(input_sentence)  # <mask>: 1279 27932 29\n",
    "    input_ids = bart.task.source_dictionary.encode_line('<s> ' + bpe_code.replace('1279 27932 29', '<mask>'), \n",
    "                                                        append_eos=True).long()\n",
    "    input_ids = input_ids.unsqueeze(0).cuda()\n",
    "    src_lengths = torch.sum(input_ids != 1, dim=1)\n",
    "    return input_ids, src_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(decoder, encoder_out, batch_size=1, tgt_tokens=None, min_decode_step=1, max_decode_step=100, pad_id=1, eos_id=2, verbose=True):\n",
    "    init_input = torch.tensor([[2, 0]] * batch_size, dtype=torch.long).cuda()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    token_probs, tokens = [], []\n",
    "\n",
    "    for step in range(max_decode_step):\n",
    "        decoder_outputs = decoder(init_input, encoder_out, features_only=False)\n",
    "        logits = decoder_outputs[0][:, -1, :]  # [batch_size, vocab]\n",
    "        \n",
    "        if step + 1 < min_decode_step:\n",
    "            logits[:, eos_id] = -math.inf\n",
    "        logits[:, pad_id], logits[:, 0] = -math.inf, -math.inf  # never select pad, start token\n",
    "\n",
    "        probs = softmax(logits)\n",
    "        assert logits.shape == probs.shape\n",
    "        attn = decoder_outputs[1]['attn'][0]  # [batch_size, prev_token_len, src_token_len]\n",
    "        assert logits.dim() == 2 and attn.dim() == 3\n",
    "\n",
    "        if tgt_tokens is not None:\n",
    "            selected_token = tgt_tokens[step].unsqueeze(0)\n",
    "        else:\n",
    "            value, indices = torch.topk(probs, 5, dim=1)\n",
    "            selected_token = indices[:, 0]\n",
    "\n",
    "        init_input = torch.cat([init_input, selected_token.unsqueeze(1)], dim=-1)\n",
    "        token, prob = decode_func(selected_token), probs.squeeze()[selected_token.item()].item()\n",
    "        \n",
    "        if selected_token.item() == eos_id:\n",
    "            break\n",
    "        elif verbose:\n",
    "            print(\"- {:02d}: {} ({:.2f})\".format(step, token, prob), end='\\n')\n",
    "\n",
    "        token_probs.append(prob)\n",
    "        tokens.append(token)\n",
    "\n",
    "    return init_input, tokens, token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(position, tokens, probs, entity):\n",
    "    \"\"\"Get probability of the given target.\n",
    "\n",
    "    Args:\n",
    "        position: (start, end)\n",
    "        tokens: ['The', ' Archbishop', ' of', ...]\n",
    "        probs: [0.50, 0.49, 0.88, ...]\n",
    "        entity: Rodgers\n",
    "    \"\"\"\n",
    "    assert len(tokens) == len(probs)\n",
    "    \n",
    "    end_pointer, end_pos = 0, []\n",
    "    for t in tokens:\n",
    "        end_pointer += len(t)\n",
    "        end_pos.append(end_pointer)\n",
    "    \n",
    "    assert position[1] in end_pos\n",
    "    last_index = end_pos.index(position[1])\n",
    "    indexes = [last_index]\n",
    "    total_length = len(tokens[last_index])\n",
    "    \n",
    "    while total_length < (position[1] - position[0]):\n",
    "        last_index -= 1\n",
    "        assert last_index >= 0\n",
    "        indexes.append(last_index)\n",
    "        total_length += len(tokens[last_index])\n",
    "    \n",
    "    indexes.reverse()\n",
    "    \n",
    "    generated = ''.join([tokens[i] for i in indexes])\n",
    "    assert entity in generated, 'entity: {}; prob calculated: {}'.format(entity, generated)\n",
    "    \n",
    "    prob = 1.0\n",
    "    for i in indexes:\n",
    "        prob *= probs[i]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmlm_probability(bart_model, sentence, masked_sentence, position, entity, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input,\n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=bart_model.encode(sentence)[1:].cuda(),\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == sentence, '- generated: {}\\n- target: {}'.format(decode_func(masked_output_ids[0]), \n",
    "                                                                                                 sentence)\n",
    "    assert ''.join(masked_tokens) == sentence, '- generated: {}\\n- target: {}'.format(''.join(masked_tokens), \n",
    "                                                                                      sentence)\n",
    "    \n",
    "    return get_probability(position, masked_tokens, masked_token_probs, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_probability(bart_model, sentence, masked_sentence, position, entity, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input,\n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=bart_model.encode(sentence)[1:].cuda(),\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == sentence, '{}; {}'.format(decode_func(masked_output_ids[0]), sentence)\n",
    "\n",
    "    return get_probability(position, masked_tokens, masked_token_probs, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmlm_generate(bart_model, masked_sentence, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input, \n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=None,\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    \n",
    "    return decode_func(masked_output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_generate(bart_model, masked_sentence):\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input, \n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=None,\n",
    "                                       verbose=False)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    \n",
    "    return decode_func(masked_output_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 9444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\n",
      "[{'start': 35, 'end': 39, 'label': 'ORG'}, {'start': 75, 'end': 82, 'label': 'ORG'}]\n",
      "\n",
      "Twin-to-twin transfusion syndrome (<mask>) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\n",
      "- entity: TTTS\n",
      "- prior: 0.0003767434973269701\n",
      "- posterior: 0.7491459846496582\n",
      "- ratio: 0.749 / 0.000 = 1937.062\n",
      "\n",
      "Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in <mask> in a bid to save the lives of babies born with the condition.\n",
      "- entity: Cardiff\n",
      "- prior: 0.0011692047119140625\n",
      "- posterior: 0.1407470703125\n",
      "- ratio: 0.141 / 0.001 = 119.358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = xsum_source[INDEX]\n",
    "target = \"Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\"\n",
    "print(target)\n",
    "\n",
    "ent_parts = nlp(target).to_json()['ents']\n",
    "print(ent_parts, end='\\n\\n')\n",
    "\n",
    "for e in ent_parts:\n",
    "    entity = target[e['start']: e['end']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "        prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "        print(target[0: e['start']] + '<mask>' + target[e['end']:])\n",
    "#         print('- prior: {}'.format(prior_generate(bart, masked_hypothesis)))\n",
    "\n",
    "        masked_hypothesis = target[0: e['start']] + '###' + target[e['end']:]\n",
    "        masked_hypothesis = '<s> ' + masked_hypothesis + ' <\\s> ' + source\n",
    "        posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                         '<s> ' + target,\n",
    "                                         masked_hypothesis,\n",
    "                                         (e['start'] + 4, e['end'] + 4),\n",
    "                                         entity, verbose=False)\n",
    "#         print(target[0: e['start']] + '###' + target[e['end']:])\n",
    "#         print('- posterior: {}'.format(cmlm_generate(finetuned_bart, masked_hypothesis, verbose=False)))\n",
    "\n",
    "        print('- entity: {}'.format(entity))\n",
    "        print('- prior: {}'.format(prior))\n",
    "        print('- posterior: {}'.format(posterior))\n",
    "        print('- ratio: {:.3f} / {:.3f} = {:.3f}'.format(posterior, prior, posterior / (prior + 1e-5)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Google Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data_path = '../Dataset/entity_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data = json.load(open(google_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(google_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(raw_doc):\n",
    "    TRIVIAL_SENTS = [\n",
    "        'Share this with',\n",
    "        'Copy this link',\n",
    "        'These are external links and will open in a new window',\n",
    "    ]\n",
    "    \n",
    "    raw_doc = raw_doc.strip()\n",
    "    raw_doc_sents = raw_doc.split('\\n')\n",
    "    \n",
    "    start_signal = False\n",
    "    filtered_sentences = []\n",
    "    for s in raw_doc_sents: \n",
    "        if start_signal:\n",
    "            filtered_sentences.append(s)\n",
    "        elif len(s.split()) > 1 and s not in TRIVIAL_SENTS:\n",
    "            start_signal = True\n",
    "            filtered_sentences.append(s)\n",
    "            \n",
    "    return ' '.join(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(bbcid):\n",
    "    folder = '/home/ml/cadencao/XSum/xsum-preprocessed/document/'\n",
    "    file_path = folder + '{}.document'.format(bbcid)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        return process_document(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'France \\'s Dubuisson carded a 67 to tie with overnight leader Van Zyl of South Africa on 16 under par . McIlroy carded a third straight five under - par 67 to move to 15 under par with Thailand \\'s Kiradech Aphibarnrat . The world number three \\'s round included an eagle on the 12th as he bids to win his first title since May . \" The 67s I \\'ve shot this week have all been a little different and I feel like I \\'ve played within myself for all of them , \" said four - time major winner McIlroy of Northern Ireland . \" I feel there \\'s a low round out there for me and hopefully it \\'s tomorrow . \" McIlroy was level par for the day after 10 holes , dropping his first shots of the week by three - putting the third and 10th , the latter mistake prompting the 26 - year - old to throw his putter at his bag . But he hit back with a birdie on the par - five 11th and a towering four iron from 229 yards on the 13th set up an eagle from just four feet . The former world number one ruptured a ligament in his left ankle during a game of football with friends in July , ruling him out of several tournaments . But he returned in time to unsuccessfully defend his US PGA title at Whistling Straits in August and played in three of the FedEx Cup play - off events before starting the new PGA Tour season with a tie for 26th in the Frys.com Open in California . He is targeting a third Race to Dubai title in four years and leads England \\'s Danny Willett by 271 , 214 points with three events remaining after the Turkish Open . English pair Chris Wood ( - 13 ) and Richard Bland ( - 12 ) who were tied for second overnight are fifth and seventh respectively .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_document(34687720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 140/500 [40:10<2:14:19, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- document 33928888 does not exist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 157/500 [45:02<1:38:24, 17.21s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "- generated: <s> Mae Gan O Gan Yng Nghymru Wedi Cael EU R AR ��l I 'R du Yng Nghymru .\n- target: <s> Mae Gan O Gan Yng Nghymru Wedi Cael EU R AR Ôl I 'R du Yng Nghymru .",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c270ec165d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                  \u001b[0mmasked_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                  \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                                  entity, verbose=False)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#                 print('- posterior int: {}'.format(masked_hypothesis))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#                 print('- posterior out: {}'.format(cmlm_generate(finetuned_bart, masked_input, verbose=False)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b1a842655553>\u001b[0m in \u001b[0;36mget_cmlm_probability\u001b[0;34m(bart_model, sentence, masked_sentence, position, entity, verbose)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                                                                  sentence)\n\u001b[1;32m     11\u001b[0m     assert ''.join(masked_tokens) == sentence, '- generated: {}\\n- target: {}'.format(''.join(masked_tokens), \n\u001b[0;32m---> 12\u001b[0;31m                                                                                       sentence)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_token_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: - generated: <s> Mae Gan O Gan Yng Nghymru Wedi Cael EU R AR ��l I 'R du Yng Nghymru .\n- target: <s> Mae Gan O Gan Yng Nghymru Wedi Cael EU R AR Ôl I 'R du Yng Nghymru ."
     ]
    }
   ],
   "source": [
    "for bbcid in tqdm(google_data.keys()):\n",
    "    try:\n",
    "        source = read_document(bbcid)\n",
    "    except:\n",
    "        print('- document {} does not exist!'.format(bbcid))\n",
    "        continue\n",
    "    \n",
    "#     print(bbcid)\n",
    "    for system in google_data[bbcid]:\n",
    "        target = google_data[bbcid][system]['summary_upper']\n",
    "        \n",
    "        for e in google_data[bbcid][system]['ents']:\n",
    "            try:\n",
    "                entity = target[e['start']: e['end']]\n",
    "                assert entity == e['ent']\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "                    prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "    #                 print('- prior int: {}'.format(target[0: e['start']] + '<mask>' + target[e['end']:]))\n",
    "    #                 print('- prior out: {}'.format(prior_generate(bart, masked_hypothesis)))\n",
    "\n",
    "                    masked_hypothesis = '<s> ' + target[0: e['start']] + '###' + target[e['end']:]\n",
    "                    masked_input = masked_hypothesis + ' <\\s> ' + source\n",
    "                    posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                                     '<s> ' + target,\n",
    "                                                     masked_input,\n",
    "                                                     (e['start'] + 4, e['end'] + 4),\n",
    "                                                     entity, verbose=False)\n",
    "    #                 print('- posterior int: {}'.format(masked_hypothesis))\n",
    "    #                 print('- posterior out: {}'.format(cmlm_generate(finetuned_bart, masked_input, verbose=False)))\n",
    "\n",
    "    #                 print('- entity: {}'.format(entity))\n",
    "    #                 print('- prior: {}'.format(prior))\n",
    "    #                 print('- posterior: {}'.format(posterior))\n",
    "    #                 print('- ratio: {:.3f} / {:.3f} = {:.3f}'.format(posterior, prior, posterior / (prior + 1e-5)))\n",
    "    #                 print('- label: {}'.format(e['label']))\n",
    "    #                 print()\n",
    "\n",
    "                    e['prior'], e['posterior'] = prior, posterior\n",
    "            except:\n",
    "                print('- error appears!')\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_with_probability(prior, posterior):\n",
    "    if posterior > 0.1 or prior > 0.2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = [], []\n",
    "\n",
    "for bbcid in google_data.keys():\n",
    "    for system in google_data[bbcid]:\n",
    "        for e in google_data[bbcid][system]['ents']:\n",
    "            if 'prior' in e and e['label'] in [0, 1, 2]:\n",
    "                pred_labels.append(get_label_with_probability(e['prior'], e['posterior']))\n",
    "                if e['label'] == 0 or e['label'] == 1:\n",
    "                    true_labels.append(0)\n",
    "                else:\n",
    "                    true_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bbcid in google_data.keys():\n",
    "#     for system in google_data[bbcid]:\n",
    "#         if system == 'Gold':\n",
    "#             continue\n",
    "#         for e in google_data[bbcid][system]['ents']:\n",
    "#             if e['label'] == -1:\n",
    "#                 print('{} - {}'.format(bbcid, system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Non-hallucination       0.61      0.65      0.63       928\n",
      "    Hallucination       0.55      0.51      0.53       783\n",
      "\n",
      "         accuracy                           0.59      1711\n",
      "        macro avg       0.58      0.58      0.58      1711\n",
      "     weighted avg       0.58      0.59      0.58      1711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, pred_labels, target_names=['Non-hallucination', 'Hallucination']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
