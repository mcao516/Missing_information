{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fairseq.models.bart import BARTModel\n",
    "from utils import read_lines, get_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bart = BARTModel.from_pretrained('/home/ml/users/cadencao/Downloads/BART_models/bart.large.xsum',\n",
    "                                           checkpoint_file='model.pt',\n",
    "                                           data_name_or_path='/home/ml/users/cadencao/Downloads/BART_models/bart.large.xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fine-tuned bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "finetuned_bart.cuda()\n",
    "finetuned_bart.eval()\n",
    "finetuned_bart.half()\n",
    "print('- fine-tuned bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/ml/users/cadencao/Downloads/BART_models/bart.large',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/ml/users/cadencao/Downloads/BART_models/bart.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_encoder = finetuned_bart.model.encoder\n",
    "bart_decoder = finetuned_bart.model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fairseq.models.transformer.TransformerEncoder'>\n",
      "<class 'fairseq.models.transformer.TransformerDecoder'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bart_encoder))\n",
    "print(type(bart_decoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/ml/users/cadencao/XSum/fairseq_files/test.source'\n",
    "target_path = '/home/ml/users/cadencao/XSum/fairseq_files/test.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, append_bos=False, append_eos=True, left_pad=True):\n",
    "    \"\"\"BPE-encode a sentence (or multiple sentences).\n",
    "\n",
    "    Args:\n",
    "        input_str (str or List[str]): input sentence to be tokenized.\n",
    "        append_bos (bool): self-explained.\n",
    "        append_eos (bool): self-explained.\n",
    "\n",
    "    Return:\n",
    "        input_ids (torch.Tensor): [batch_size, length]\n",
    "        src_lengths (torch.Tensor): [batch_size]\n",
    "    \"\"\"\n",
    "    if type(input_str) == type(''):\n",
    "        input_str = [input_str]\n",
    "\n",
    "    input_ids = []\n",
    "    for ins in input_str:\n",
    "        tokens = bart.bpe.encode(ins)  # <mask>: 1279 27932 29\n",
    "        calibration = sum([append_bos, append_eos])\n",
    "        if len(tokens.split(\" \")) > min(bart.max_positions) - calibration:\n",
    "            tokens = \" \".join(tokens.split(\" \")[: min(bart.max_positions) - calibration])\n",
    "\n",
    "        tokens = \"<s> \" + tokens if append_bos else tokens\n",
    "        tokens = tokens + \" </s>\" if append_eos else tokens\n",
    "        ids = bart.task.source_dictionary.encode_line(tokens, append_eos=False).long()\n",
    "        input_ids.append(ids)\n",
    "\n",
    "    input_ids = collate_tokens(input_ids, pad_idx=1, left_pad=left_pad).cuda()\n",
    "    input_lengths = torch.sum(input_ids != 1, dim=1).cuda()\n",
    "\n",
    "    return input_ids, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4030, 11955,  1044, 13015,  9122,   161,    37,    34,    57,  7345,\n",
       "             39,   499,   708,    19,   103,     9,    39,   949,   472,     4,\n",
       "              2]], device='cuda:0'), tensor([21], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"New Celtic manager Brendan Rodgers says he has been discussing his future plans with some of his senior players.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"New Celtic manager Brendan Rodgers says he has been discussing his future plans with some of his senior players.\"\n",
    "# [0,  4030, 11955,  1044, 13015,  9122,   161,    37,    34,    57,\n",
    "#  7345,    39,   499,   708,    19,   103,     9,    39,   949,   472, 4,     2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(decoder, encoder_out, tgt_tokens=None, min_decode_step=10, max_decode_step=60, pad_id=1, eos_id=2, verbose=True):\n",
    "    init_input = torch.tensor([[2, 0]] * src_tokens.shape[0], dtype=torch.long).cuda()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    token_probs, tokens = [], []\n",
    "\n",
    "    for step in range(max_decode_step):\n",
    "        decoder_outputs = decoder(init_input, encoder_out, features_only=False)\n",
    "        logits = decoder_outputs[0][:, -1, :]  # [batch_size, vocab]\n",
    "        \n",
    "        if step + 1 < min_decode_step:\n",
    "            logits[:, eos_id] = -math.inf\n",
    "        logits[:, pad_id], logits[:, 0] = -math.inf, -math.inf  # never select pad, start token\n",
    "\n",
    "        probs = softmax(logits)\n",
    "        assert logits.shape == probs.shape\n",
    "        attn = decoder_outputs[1]['attn'][0]  # [batch_size, prev_token_len, src_token_len]\n",
    "        assert logits.dim() == 2 and attn.dim() == 3\n",
    "\n",
    "        if tgt_tokens is not None:\n",
    "            selected_token = tgt_tokens[step].unsqueeze(0)\n",
    "        else:\n",
    "            value, indices = torch.topk(probs, 5, dim=1)\n",
    "            selected_token = indices[:, 0]\n",
    "\n",
    "#             if step == 1:\n",
    "#                 selected_token = indices[:, 0]\n",
    "#             elif step == 10:\n",
    "#                 selected_token = indices[:, 0]\n",
    "#             elif step == 12:\n",
    "#                 selected_token = indices[:, 0]\n",
    "\n",
    "        init_input = torch.cat([init_input, selected_token.unsqueeze(1)], dim=-1)\n",
    "        token, prob = decode_func(selected_token), probs.squeeze()[selected_token.item()].item()\n",
    "        token_probs.append(prob)\n",
    "        tokens.append(token)\n",
    "\n",
    "        if selected_token.item() == eos_id:\n",
    "            break\n",
    "        elif verbose:\n",
    "            print(\"- {:02d}: {} ({:.2f})\".format(step, token, prob), end='\\n')\n",
    "\n",
    "    return init_input, tokens, token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_mask(input_sentence):\n",
    "    bpe_code = bart.bpe.encode(input_sentence)  # <mask>: 1279 27932 29\n",
    "    input_ids = bart.task.source_dictionary.encode_line('<s> ' + bpe_code.replace('1279 27932 29', '<mask>'), \n",
    "                                                        append_eos=True).long()\n",
    "    input_ids = input_ids.unsqueeze(0).cuda()\n",
    "    src_lengths = torch.sum(input_ids != 1, dim=1)\n",
    "    return input_ids, src_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "src_tokens, src_lengths = tokenize(xsum_source[INDEX])\n",
    "\n",
    "# encode & decode\n",
    "encoder_out = finetuned_bart.model.encoder(src_tokens, src_lengths=src_lengths)\n",
    "output_ids, tokens, token_probs = generate_sequence(bart_decoder, encoder_out, verbose=False)\n",
    "\n",
    "print(output_ids.shape)\n",
    "\n",
    "hypothesis = decode_func(output_ids[0])\n",
    "print(hypothesis + '\\n')\n",
    "\n",
    "ent_parts = ' '.join([e.text for e in nlp(hypothesis).ents]).split()\n",
    "ent_parts = [e.text for e in nlp(hypothesis).ents]\n",
    "for entity in ent_parts:\n",
    "    masked_summary = hypothesis.replace(entity, '<mask>')\n",
    "\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_summary)\n",
    "    masked_outputs = generate_sequence(bart.model.decoder,\n",
    "                                       bart.model.encoder(masked_input,\n",
    "                                                          src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=output_ids[0][2:],\n",
    "                                       verbose=False)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == hypothesis\n",
    "\n",
    "    posterior = get_probability(entity, tokens, token_probs)\n",
    "    prior = get_probability(entity, masked_tokens, masked_token_probs)\n",
    "    \n",
    "    print('- entity: {}'.format(entity))\n",
    "    print('- prior: {}'.format(prior))\n",
    "    print('- posterior: {}'.format(posterior))\n",
    "    print('- ratio: {:.3f} / {:.3f} = {:.3f}'.format(posterior, prior, posterior / (prior + 1e-5)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ulster centre Stuart McCloskey has signed a contract extension which will see him remain at Kingspan Stadium until the summer of 2019.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_target[INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 271])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([271, 1, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
