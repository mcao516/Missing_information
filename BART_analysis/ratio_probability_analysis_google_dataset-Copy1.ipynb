{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fairseq.models.bart import BARTModel\n",
    "from utils import read_lines\n",
    "\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = json.load(open('path_config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bart = BARTModel.from_pretrained(PATH['xsum_cmlm_bos'],\n",
    "                                           checkpoint_file='checkpoint_best.pt',\n",
    "                                           data_name_or_path=PATH['data_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart = BARTModel.from_pretrained(PATH['bart.large'],\n",
    "#                                  checkpoint_file='model.pt',\n",
    "#                                  data_name_or_path=PATH['bart.large'])\n",
    "\n",
    "bart = BARTModel.from_pretrained(PATH['cnndm_cmlm_cedar'],\n",
    "                                       checkpoint_file='checkpoint_best.pt',\n",
    "                                       data_name_or_path=PATH['data_name_or_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = PATH['xsum_fariseq'] + '/test.source'\n",
    "target_path = PATH['xsum_fariseq'] + '/test.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSequenceGenerator:\n",
    "    \"\"\"Conditional sequence generator for calculating prior and posterior probability.\"\"\"\n",
    "    def __init__(self, bart):\n",
    "        self.bart = bart\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "        \n",
    "        self.encode_func = bart.encode\n",
    "        self.decode_func = bart.decode\n",
    "        self.max_positions = bart.max_positions\n",
    "        self.encode_line = bart.task.source_dictionary.encode_line\n",
    "        \n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"Set BART model to evaluation mode.\"\"\"\n",
    "        self.bart.cuda()\n",
    "        self.bart.eval()\n",
    "        self.bart.half()\n",
    "        \n",
    "    def tokenize(self, input_str, append_bos=False, append_eos=True, left_pad=True):\n",
    "        \"\"\"BPE-encode a sentence (or multiple sentences).\n",
    "\n",
    "        Args:\n",
    "            input_str (str or List[str]): input sentence to be tokenized.\n",
    "            append_bos (bool): self-explained.\n",
    "            append_eos (bool): self-explained.\n",
    "\n",
    "        Return:\n",
    "            input_ids (torch.Tensor): [batch_size, length]\n",
    "            src_lengths (torch.Tensor): [batch_size]\n",
    "        \"\"\"\n",
    "        if type(input_str) == type(''):\n",
    "            input_str = [input_str]\n",
    "\n",
    "        input_ids = []\n",
    "        for ins in input_str:\n",
    "            tokens = self.bart.bpe.encode(ins)  # <mask>: 1279 27932 29\n",
    "            calibration = sum([append_bos, append_eos])\n",
    "            if len(tokens.split(\" \")) > min(self.max_positions) - calibration:\n",
    "                tokens = \" \".join(tokens.split(\" \")[: min(self.max_positions) - calibration])\n",
    "\n",
    "            tokens = \"<s> \" + tokens if append_bos else tokens\n",
    "            tokens = tokens + \" </s>\" if append_eos else tokens\n",
    "            ids = self.encode_line(tokens, append_eos=False).long()\n",
    "            input_ids.append(ids)\n",
    "\n",
    "        input_ids = collate_tokens(input_ids, pad_idx=1, left_pad=left_pad).cuda()\n",
    "        input_lengths = torch.sum(input_ids != 1, dim=1).cuda()\n",
    "\n",
    "        return input_ids, input_lengths\n",
    "    \n",
    "    def tokenize_with_mask(self, input_str):\n",
    "        \"\"\"Tokenize sentence with a special <mask> token in it.\n",
    "\n",
    "        Args:\n",
    "            input_str (str or List[str]): input sentence to be tokenized.\n",
    "\n",
    "        Return:\n",
    "            input_ids (torch.Tensor): [batch_size, length]\n",
    "            src_lengths (torch.Tensor): [batch_size]\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(input_str, return_tensors='pt', padding=True)['input_ids'].cuda()\n",
    "        input_lengths = torch.sum(input_ids != 1, dim=1).cuda()\n",
    "        return input_ids, input_lengths\n",
    "    \n",
    "    def generate(self, src_input, tgt_input=None):\n",
    "        \"\"\"Conditional generation.\"\"\"\n",
    "        input_ids, lengths = self.tokenize(src_input, append_bos=False) \n",
    "        \n",
    "        target_ids = None\n",
    "        if tgt_input is not None:\n",
    "            assert len(src_input) == len(tgt_input), \"source & target length should match.\"\n",
    "            target_ids, _ = self.tokenize(tgt_input, append_bos=False, left_pad=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_output = self.encode_sequence(input_ids, lengths)\n",
    "            decoder_output = self.decode_sequence(encoder_output, \n",
    "                                                  target_ids=target_ids,\n",
    "                                                  prefix_tokens=[2])\n",
    "        return decoder_output\n",
    "    \n",
    "    def mask_filling(self, src_input, tgt_input=None):\n",
    "        \"\"\"\n",
    "        Filling the mask in sentence(s).\n",
    "        \"\"\"\n",
    "        input_ids, lengths = self.tokenize_with_mask(src_input)\n",
    "        \n",
    "        target_ids = None\n",
    "        if tgt_input is not None:\n",
    "            assert len(src_input) == len(tgt_input), \"source & target length should match.\"\n",
    "            target_ids, _ = self.tokenize(tgt_input, left_pad=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_output = self.encode_sequence(input_ids, lengths)\n",
    "            decoder_output = self.decode_sequence(encoder_output, \n",
    "                                                  target_ids=target_ids,\n",
    "                                                  prefix_tokens=[2, 0])\n",
    "        return decoder_output\n",
    "    \n",
    "    def encode_sequence(self, input_ids, lengths):\n",
    "        return self.bart.model.encoder(input_ids, src_lengths=lengths)\n",
    "        \n",
    "    def decode_sequence(\n",
    "        self,\n",
    "        encoder_out,\n",
    "        target_ids=None,\n",
    "        min_decode_step=3,\n",
    "        max_decode_step=100,\n",
    "        pad_id=1,\n",
    "        eos_id=2,\n",
    "        prefix_tokens=[2, 0],\n",
    "    ):\n",
    "        batch_size = encoder_out['encoder_padding_mask'][0].shape[0]\n",
    "        init_input = torch.tensor([prefix_tokens] * batch_size, dtype=torch.long).cuda()\n",
    "        token_probs, tokens = None, [[] for i in range(batch_size)]\n",
    "        end_mask = torch.tensor([False] * batch_size).cuda()\n",
    "\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for step in range(max_decode_step):\n",
    "            decoder_outputs = self.bart.model.decoder(init_input, encoder_out, features_only=False)\n",
    "            logits = decoder_outputs[0][:, -1, :]  # logits: [batch_size, vocab]\n",
    "            attn = decoder_outputs[1]['attn'][0]  # [batch_size, prev_token_len, src_token_len]\n",
    "\n",
    "            if step + 1 < min_decode_step:\n",
    "                logits[:, eos_id] = -math.inf  # mask <EOS> token when within minimal step\n",
    "            logits[:, pad_id], logits[:, 0] = -math.inf, -math.inf  # never select <PAD> & <BOS> token\n",
    "            probs = softmax(logits)  # probs: [batch_size, vocab]\n",
    "\n",
    "            # select tokens\n",
    "            if target_ids is not None:\n",
    "                selected_token = target_ids[:, step]\n",
    "            else:\n",
    "                value, indices = torch.topk(probs, 5, dim=1)\n",
    "                selected_token = indices[:, 0]\n",
    "\n",
    "            selected_token = selected_token.masked_fill(end_mask, pad_id)\n",
    "            init_input = torch.cat([init_input, selected_token.unsqueeze(1)], dim=-1)\n",
    "            \n",
    "            probs = torch.gather(probs, 1, selected_token.unsqueeze(1)).detach()\n",
    "            probs = probs.masked_fill(end_mask.unsqueeze(1), 1.0)\n",
    "            \n",
    "            # str & probability\n",
    "            token_probs = probs if token_probs is None else torch.cat([token_probs, probs], dim=-1)\n",
    "            for t, s in zip(tokens, selected_token):\n",
    "                t.append(self.decode_func(s.unsqueeze(0)) if s.item() != pad_id else '<pad>')\n",
    "            \n",
    "            # stop generation when all finished\n",
    "            end_mask = torch.logical_or(end_mask, selected_token == eos_id) \n",
    "            if end_mask.sum().item() == batch_size:\n",
    "                break\n",
    "\n",
    "        return init_input, tokens, token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(position, tokens, probs, entity):\n",
    "    \"\"\"Calculate the probability of a span.\n",
    "\n",
    "    Args:\n",
    "        position: (start, end)\n",
    "        tokens: ['The', ' Archbishop', ' of', ...]\n",
    "        probs: [0.50, 0.49, 0.88, ...]\n",
    "        entity: Rodgers\n",
    "    \"\"\"\n",
    "    assert len(tokens) == len(probs), \"Tokens and token probabilities does not match.\"\n",
    "    \n",
    "    end_pointer, end_pos = 0, []\n",
    "    for t in tokens:\n",
    "        end_pointer += len(t)\n",
    "        end_pos.append(end_pointer)\n",
    "    \n",
    "    assert position[1] in end_pos, \"- {}\\n- {}\\n- {}\\n- {}\\n- {}\\n\".format(position, tokens, probs, entity, end_pos)\n",
    "    last_index = end_pos.index(position[1])\n",
    "    indexes = [last_index]\n",
    "    total_length = len(tokens[last_index])\n",
    "    \n",
    "    while total_length < (position[1] - position[0]):\n",
    "        last_index -= 1\n",
    "        assert last_index >= 0\n",
    "        indexes.append(last_index)\n",
    "        total_length += len(tokens[last_index])\n",
    "    \n",
    "    indexes.reverse()\n",
    "    \n",
    "    generated = ''.join([tokens[i] for i in indexes])\n",
    "    assert entity in generated, 'entity: {}; span: {}'.format(entity, generated)\n",
    "    \n",
    "    prob = 1.0\n",
    "    for i in indexes:\n",
    "        prob *= probs[i]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmlm_probability(generator, src_input, tgt_input, position, entity):\n",
    "    outputs = generator.generate(src_input, tgt_input=tgt_input)\n",
    "    init_input, tokens, token_probs = outputs\n",
    "    \n",
    "    probs = []\n",
    "    for p, tok, tokp, e in zip(position, tokens, token_probs, entity):\n",
    "        probs.append(get_probability(p, tok, tokp, e).item())\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_probability(generator, src_input, tgt_input, position, entity):\n",
    "    assert len(src_input) == len(tgt_input), \"source & target length should match.\"\n",
    "    decoder_output = generator.mask_filling(src_input, tgt_input)\n",
    "    init_input, tokens, token_probs = decoder_output\n",
    "    \n",
    "    probs = []\n",
    "    for p, tok, tokp, e in zip(position, tokens, token_probs, entity):\n",
    "        probs.append(get_probability(p, tok, tokp, e).item())\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Google Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data = json.load(open('../Dataset/entity_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': 'veteran classical music conductor christopher hogwood has died at the age of 83.',\n",
       " 'summary_upper': 'Veteran classical music conductor Christopher Hogwood has died at the age of 83 .',\n",
       " 'ents': [{'start': 34,\n",
       "   'end': 45,\n",
       "   'label': 0,\n",
       "   'type': 'PERSON',\n",
       "   'ent': 'Christopher'},\n",
       "  {'start': 46, 'end': 53, 'label': 0, 'type': 'PERSON', 'ent': 'Hogwood'},\n",
       "  {'start': 66,\n",
       "   'end': 79,\n",
       "   'label': 0,\n",
       "   'type': 'DATE',\n",
       "   'ent': 'the age of 83'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(google_data))\n",
    "google_data['29347895']['BERTS2S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(raw_doc):\n",
    "    TRIVIAL_SENTS = [\n",
    "        'Share this with',\n",
    "        'Copy this link',\n",
    "        'These are external links and will open in a new window',\n",
    "    ]\n",
    "    \n",
    "    raw_doc = raw_doc.strip()\n",
    "    raw_doc_sents = raw_doc.split('\\n')\n",
    "    \n",
    "    start_signal = False\n",
    "    filtered_sentences = []\n",
    "    for s in raw_doc_sents: \n",
    "        if start_signal:\n",
    "            filtered_sentences.append(s)\n",
    "        elif len(s.split()) > 1 and s not in TRIVIAL_SENTS:\n",
    "            start_signal = True\n",
    "            filtered_sentences.append(s)\n",
    "            \n",
    "    return ' '.join(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(bbcid, folder='/home/mcao610/scratch/summarization/XSum/xsum-preprocessed/document/'):\n",
    "    file_path = folder + '{}.document'.format(bbcid)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        return process_document(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_document_exist(bbcid, folder='/home/mcao610/scratch/summarization/XSum/xsum-preprocessed/document/'):\n",
    "    file_path = folder + '{}.document'.format(bbcid)\n",
    "\n",
    "    return os.path.exists(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'France \\'s Dubuisson carded a 67 to tie with overnight leader Van Zyl of South Africa on 16 under par . McIlroy carded a third straight five under - par 67 to move to 15 under par with Thailand \\'s Kiradech Aphibarnrat . The world number three \\'s round included an eagle on the 12th as he bids to win his first title since May . \" The 67s I \\'ve shot this week have all been a little different and I feel like I \\'ve played within myself for all of them , \" said four - time major winner McIlroy of Northern Ireland . \" I feel there \\'s a low round out there for me and hopefully it \\'s tomorrow . \" McIlroy was level par for the day after 10 holes , dropping his first shots of the week by three - putting the third and 10th , the latter mistake prompting the 26 - year - old to throw his putter at his bag . But he hit back with a birdie on the par - five 11th and a towering four iron from 229 yards on the 13th set up an eagle from just four feet . The former world number one ruptured a ligament in his left ankle during a game of football with friends in July , ruling him out of several tournaments . But he returned in time to unsuccessfully defend his US PGA title at Whistling Straits in August and played in three of the FedEx Cup play - off events before starting the new PGA Tour season with a tie for 26th in the Frys.com Open in California . He is targeting a third Race to Dubai title in four years and leads England \\'s Danny Willett by 271 , 214 points with three events remaining after the Turkish Open . English pair Chris Wood ( - 13 ) and Richard Bland ( - 12 ) who were tied for second overnight are fifth and seventh respectively .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_document(34687720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clm_inputs(source, target, ent_parts=None):\n",
    "    \"\"\"For Conditional Language Model.\"\"\"\n",
    "    if ent_parts is None:\n",
    "        ent_parts = nlp(target).to_json()['ents']\n",
    "    \n",
    "    entities, positions = [], []\n",
    "    inputs, targets = [], []\n",
    "\n",
    "    for e in ent_parts:\n",
    "        inputs.append(source)\n",
    "        targets.append(target)\n",
    "        positions.append((e['start'], e['end']))\n",
    "        entities.append(target[e['start']: e['end']])\n",
    "\n",
    "    return inputs, targets, positions, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mlm_inputs(source, target, ent_parts=None):\n",
    "    \"\"\"For Masked Language Model.\"\"\"\n",
    "    if ent_parts is None:\n",
    "        ent_parts = nlp(target).to_json()['ents']\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    positions, entities = [], []\n",
    "\n",
    "    for e in ent_parts:\n",
    "        inputs.append(target[0: e['start']] + '<mask>' + target[e['end']:])\n",
    "        targets.append(target)\n",
    "        entities.append(target[e['start']: e['end']])\n",
    "        positions.append((e['start'], e['end']))\n",
    "    \n",
    "    return inputs, targets, positions, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cmlm_inputs(source, target, ent_parts=None):\n",
    "    \"\"\"For Masked Language Model.\"\"\"\n",
    "    if ent_parts is None:\n",
    "        ent_parts = nlp(target).to_json()['ents']\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    positions, entities = [], []\n",
    "\n",
    "    for e in ent_parts:\n",
    "        masked_hypothesis = target[0: e['start']] + '###' + target[e['end']:]\n",
    "        masked_hypothesis = '<s> ' + masked_hypothesis + ' <\\s> ' + source\n",
    "        inputs.append(masked_hypothesis)\n",
    "        targets.append('<s> ' + target)\n",
    "        \n",
    "        entities.append(target[e['start']: e['end']])\n",
    "        positions.append((e['start'] + 4, e['end'] + 4))\n",
    "\n",
    "    return inputs, targets, positions, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'veteran classical music conductor christopher hogwood has died at the age of 83.',\n",
       " 'summary_upper': 'Veteran classical music conductor Christopher Hogwood has died at the age of 83 .',\n",
       " 'ents': [{'start': 34,\n",
       "   'end': 45,\n",
       "   'label': 0,\n",
       "   'type': 'PERSON',\n",
       "   'ent': 'Christopher'},\n",
       "  {'start': 46, 'end': 53, 'label': 0, 'type': 'PERSON', 'ent': 'Hogwood'},\n",
       "  {'start': 66,\n",
       "   'end': 79,\n",
       "   'label': 0,\n",
       "   'type': 'DATE',\n",
       "   'ent': 'the age of 83'}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_data['29347895']['BERTS2S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model = ConditionalSequenceGenerator(bart)\n",
    "posterior_model = ConditionalSequenceGenerator(finetuned_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [34:09<00:00,  4.10s/it]\n"
     ]
    }
   ],
   "source": [
    "unsuccessful_ids = []\n",
    "for bbcid in tqdm(google_data.keys()):\n",
    "    if bbcid == '39553812': continue\n",
    "    if check_document_exist(bbcid):    \n",
    "        for system in google_data[bbcid]:\n",
    "            source = read_document(bbcid)\n",
    "            target = google_data[bbcid][system]['summary_upper']\n",
    "\n",
    "            if len(google_data[bbcid][system]['ents']) > 0:\n",
    "                pro = prepare_cmlm_inputs(source, target, google_data[bbcid][system]['ents'])\n",
    "                pos = prepare_cmlm_inputs(source, target, google_data[bbcid][system]['ents'])\n",
    "                \n",
    "                prior_probs = get_cmlm_probability(prior_model, pro[0], pro[1], pro[2], pro[3])\n",
    "                posterior_probs = get_cmlm_probability(posterior_model, pos[0], pos[1], pos[2], pos[3])\n",
    "\n",
    "                assert len(prior_probs) == len(posterior_probs) == len(google_data[bbcid][system]['ents'])\n",
    "                for i in range(len(prior_probs)):\n",
    "                    google_data[bbcid][system]['ents'][i]['prior'] = prior_probs[i]\n",
    "                    google_data[bbcid][system]['ents'][i]['posterior'] = posterior_probs[i]\n",
    "    else:\n",
    "        for system in google_data[bbcid]:\n",
    "            for i in range(len(google_data[bbcid][system]['ents'])):\n",
    "                google_data[bbcid][system]['ents'][i]['prior'] = None\n",
    "                google_data[bbcid][system]['ents'][i]['posterior'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BERTS2S': {'summary': 'young people in scotland are more likely than their peers to commit violent crimes, according to new research.',\n",
       "  'summary_upper': 'Young people in Scotland are more likely than their peers to commit violent crimes , according to new research .',\n",
       "  'ents': [{'start': 16,\n",
       "    'end': 24,\n",
       "    'label': 0,\n",
       "    'type': 'GPE',\n",
       "    'ent': 'Scotland',\n",
       "    'prior': 0.1915283203125,\n",
       "    'posterior': 0.8310546875}]},\n",
       " 'TConvS2S': {'summary': 'young girls in scotland are more likely to be linked to violent crime, according to a new study.',\n",
       "  'summary_upper': 'Young girls in Scotland are more likely to be linked to violent crime , according to a new study .',\n",
       "  'ents': [{'start': 15,\n",
       "    'end': 23,\n",
       "    'label': 0,\n",
       "    'type': 'GPE',\n",
       "    'ent': 'Scotland',\n",
       "    'prior': 0.1826171875,\n",
       "    'posterior': 0.364501953125}]},\n",
       " 'Gold': {'summary': \"scotland's criminal justice system punishes poorer people and makes it difficult for them to escape poverty, according to an academic study.\",\n",
       "  'summary_upper': \"Scotland 's criminal justice system punishes poorer people and makes it difficult for them to escape poverty , according to an academic study .\",\n",
       "  'ents': [{'start': 0,\n",
       "    'end': 8,\n",
       "    'label': -1,\n",
       "    'type': 'GPE',\n",
       "    'ent': 'Scotland',\n",
       "    'prior': 0.880859375,\n",
       "    'posterior': 0.88134765625}]},\n",
       " 'PtGen': {'summary': 'young people who lived in extreme poverty are more likely to be placed on statutory supervision, a study suggests.',\n",
       "  'summary_upper': 'Young people who lived in extreme poverty are more likely to be placed on statutory supervision , a study suggests .',\n",
       "  'ents': []},\n",
       " 'TranS2S': {'summary': 'young people in edinburgh are more likely to be living in violent crime than girls, according to new research.',\n",
       "  'summary_upper': 'Young people in Edinburgh are more likely to be living in violent crime than girls , according to new research .',\n",
       "  'ents': [{'start': 16,\n",
       "    'end': 25,\n",
       "    'label': 0,\n",
       "    'type': 'GPE',\n",
       "    'ent': 'Edinburgh',\n",
       "    'prior': 0.7421875,\n",
       "    'posterior': 0.058319091796875}]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_data['34802406']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(google_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(google_data, open(\"google_data_with_proba_2CMLM.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
