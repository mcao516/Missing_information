{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fairseq.models.bart import BARTModel\n",
    "from utils import read_lines, get_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bart = BARTModel.from_pretrained('/home/ml/cadencao/fairseq/checkpoints/xsum_cmlm_bos_ent',\n",
    "                                           checkpoint_file='checkpoint_best.pt',\n",
    "                                           data_name_or_path='/home/ml/cadencao/XSum/fairseq_files/xsum-bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fine-tuned bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "finetuned_bart.cuda()\n",
    "finetuned_bart.eval()\n",
    "finetuned_bart.half()\n",
    "print('- fine-tuned bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/ml/cadencao/Downloads/BART_models/bart.large',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/ml/cadencao/Downloads/BART_models/bart.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/ml/cadencao/XSum/fairseq_files/test.source'\n",
    "target_path = '/home/ml/cadencao/XSum/fairseq_files/test.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(src_input, verbose=False):\n",
    "    src_inputs = [src_input]  # list of input string\n",
    "    src_tokens = collate_tokens([encode_func(i) for i in src_inputs], pad_idx=1, left_pad=True)\n",
    "    src_tokens = src_tokens.cuda()\n",
    "    src_lengths = torch.sum(src_tokens != 1, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('- src tokens: {};\\n- src lengths: {}'.format(src_tokens.shape, src_lengths.shape))\n",
    "    return src_tokens, src_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_mask(input_sentence):\n",
    "    bpe_code = bart.bpe.encode(input_sentence)  # <mask>: 1279 27932 29\n",
    "    input_ids = bart.task.source_dictionary.encode_line('<s> ' + bpe_code.replace('1279 27932 29', '<mask>'), \n",
    "                                                        append_eos=True).long()\n",
    "    input_ids = input_ids.unsqueeze(0).cuda()\n",
    "    src_lengths = torch.sum(input_ids != 1, dim=1)\n",
    "    return input_ids, src_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(decoder, encoder_out, batch_size=1, tgt_tokens=None, min_decode_step=1, max_decode_step=100, pad_id=1, eos_id=2, verbose=True):\n",
    "    init_input = torch.tensor([[2, 0]] * batch_size, dtype=torch.long).cuda()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    token_probs, tokens = [], []\n",
    "\n",
    "    for step in range(max_decode_step):\n",
    "        decoder_outputs = decoder(init_input, encoder_out, features_only=False)\n",
    "        logits = decoder_outputs[0][:, -1, :]  # [batch_size, vocab]\n",
    "        \n",
    "        if step + 1 < min_decode_step:\n",
    "            logits[:, eos_id] = -math.inf\n",
    "        logits[:, pad_id], logits[:, 0] = -math.inf, -math.inf  # never select pad, start token\n",
    "\n",
    "        probs = softmax(logits)\n",
    "        assert logits.shape == probs.shape\n",
    "        attn = decoder_outputs[1]['attn'][0]  # [batch_size, prev_token_len, src_token_len]\n",
    "        assert logits.dim() == 2 and attn.dim() == 3\n",
    "\n",
    "        if tgt_tokens is not None:\n",
    "            selected_token = tgt_tokens[step].unsqueeze(0)\n",
    "        else:\n",
    "            value, indices = torch.topk(probs, 5, dim=1)\n",
    "            selected_token = indices[:, 0]\n",
    "\n",
    "        init_input = torch.cat([init_input, selected_token.unsqueeze(1)], dim=-1)\n",
    "        token, prob = decode_func(selected_token), probs.squeeze()[selected_token.item()].item()\n",
    "        \n",
    "        if selected_token.item() == eos_id:\n",
    "            break\n",
    "        elif verbose:\n",
    "            print(\"- {:02d}: {} ({:.2f})\".format(step, token, prob), end='\\n')\n",
    "\n",
    "        token_probs.append(prob)\n",
    "        tokens.append(token)\n",
    "\n",
    "    return init_input, tokens, token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(position, tokens, probs, entity):\n",
    "    \"\"\"Get probability of the given target.\n",
    "\n",
    "    Args:\n",
    "        position: (start, end)\n",
    "        tokens: ['The', ' Archbishop', ' of', ...]\n",
    "        probs: [0.50, 0.49, 0.88, ...]\n",
    "        entity: Rodgers\n",
    "    \"\"\"\n",
    "    assert len(tokens) == len(probs)\n",
    "    \n",
    "    end_pointer, end_pos = 0, []\n",
    "    for t in tokens:\n",
    "        end_pointer += len(t)\n",
    "        end_pos.append(end_pointer)\n",
    "    \n",
    "    assert position[1] in end_pos\n",
    "    last_index = end_pos.index(position[1])\n",
    "    indexes = [last_index]\n",
    "    total_length = len(tokens[last_index])\n",
    "    \n",
    "    while total_length < (position[1] - position[0]):\n",
    "        last_index -= 1\n",
    "        assert last_index >= 0\n",
    "        indexes.append(last_index)\n",
    "        total_length += len(tokens[last_index])\n",
    "    \n",
    "    indexes.reverse()\n",
    "    \n",
    "    generated = ''.join([tokens[i] for i in indexes])\n",
    "    assert entity in generated, 'entity: {}; prob calculated: {}'.format(entity, generated)\n",
    "    \n",
    "    prob = 1.0\n",
    "    for i in indexes:\n",
    "        prob *= probs[i]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmlm_probability(bart_model, masked_sentence, entity, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input,\n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=bart_model.encode(entity)[1:].cuda(),\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == entity\n",
    "    assert ''.join(masked_tokens) == entity\n",
    "    \n",
    "    prob = 1.0\n",
    "    for i in range(3, len(masked_token_probs)):\n",
    "        prob *= masked_token_probs[i]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_probability(bart_model, sentence, masked_sentence, position, entity, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input,\n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=bart_model.encode(sentence)[1:].cuda(),\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    assert decode_func(masked_output_ids[0]) == sentence, '{}; {}'.format(decode_func(masked_output_ids[0]), sentence)\n",
    "\n",
    "    return get_probability(position, masked_tokens, masked_token_probs, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmlm_generate(bart_model, masked_sentence, verbose=False):\n",
    "    masked_input, masked_lengths = tokenize(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input, \n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=None,\n",
    "                                       verbose=verbose)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    \n",
    "    return decode_func(masked_output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_generate(bart_model, masked_sentence):\n",
    "    masked_input, masked_lengths = tokenize_with_mask(masked_sentence)\n",
    "    masked_outputs = generate_sequence(bart_model.model.decoder,\n",
    "                                       bart_model.model.encoder(masked_input, \n",
    "                                                                src_lengths=masked_lengths),\n",
    "                                       tgt_tokens=None,\n",
    "                                       verbose=False)\n",
    "    masked_output_ids, masked_tokens, masked_token_probs = masked_outputs\n",
    "    \n",
    "    return decode_func(masked_output_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 7079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Celtic manager Brendan Rodgers has met the club's captain for the first time as he prepares for his first game in charge of the club.\n",
      "[{'start': 0, 'end': 10, 'label': 'NORP'}, {'start': 19, 'end': 34, 'label': 'PERSON'}, {'start': 70, 'end': 75, 'label': 'ORDINAL'}, {'start': 104, 'end': 109, 'label': 'ORDINAL'}]\n",
      "\n",
      "- entity: New Celtic\n",
      "- prior: 0.00017683696933090687\n",
      "- posterior: 0.2803802490234375\n",
      "- ratio: 0.280 / 0.000 = 1500.668\n",
      "\n",
      "- entity: Brendan Rodgers\n",
      "- prior: 0.05584716796875\n",
      "- posterior: 0.8109626770019531\n",
      "- ratio: 0.811 / 0.056 = 14.519\n",
      "\n",
      "- entity: first\n",
      "- prior: 0.99072265625\n",
      "- posterior: 0.9189453125\n",
      "- ratio: 0.919 / 0.991 = 0.928\n",
      "\n",
      "- entity: first\n",
      "- prior: 0.91552734375\n",
      "- posterior: 0.892578125\n",
      "- ratio: 0.893 / 0.916 = 0.975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = xsum_source[INDEX]\n",
    "target = \"New Celtic manager Brendan Rodgers has met the club's captain for the first time as he prepares for his first game in charge of the club.\"\n",
    "print(target)\n",
    "\n",
    "ent_parts = nlp(target).to_json()['ents']\n",
    "print(ent_parts, end='\\n\\n')\n",
    "\n",
    "for e in ent_parts:\n",
    "    entity = target[e['start']: e['end']]\n",
    "    \n",
    "    masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "    prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "#     print(target[0: e['start']] + '<mask>' + target[e['end']:])\n",
    "#     print('- generated: {}'.format(prior_generate(bart, masked_hypothesis)))\n",
    "    \n",
    "    masked_hypothesis = target[0: e['start']] + '###' + target[e['end']:]\n",
    "    masked_hypothesis = '<s> ' + masked_hypothesis + ' <\\s> ' + source\n",
    "    posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                     masked_hypothesis,\n",
    "                                     '<s> ' + entity, \n",
    "                                     verbose=False)\n",
    "#     print(target[0: e['start']] + '###' + target[e['end']:])\n",
    "#     print(cmlm_generate(finetuned_bart, masked_hypothesis))\n",
    "\n",
    "    print('- entity: {}'.format(entity))\n",
    "    print('- prior: {}'.format(prior))\n",
    "    print('- posterior: {}'.format(posterior))\n",
    "    print('- ratio: {:.3f} / {:.3f} = {:.3f}'.format(posterior, prior, posterior / (prior + 1e-5)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
