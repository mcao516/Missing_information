{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fairseq.models.bart import BARTModel\n",
    "from utils import read_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_bart = BARTModel.from_pretrained('/home/ml/users/cadencao/fairseq/checkpoints/xsum_cmlm_bos',\n",
    "                                           checkpoint_file='checkpoint_best.pt',\n",
    "                                           data_name_or_path='/home/ml/users/cadencao/XSum/fairseq_files/xsum-bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- fine-tuned bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "finetuned_bart.cuda()\n",
    "finetuned_bart.eval()\n",
    "finetuned_bart.half()\n",
    "print('- fine-tuned bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/ml/users/cadencao/Downloads/BART_models/bart.large',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/ml/users/cadencao/Downloads/BART_models/bart.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bart model loaded.\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- bart model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11301\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/ml/users/cadencao/XSum/fairseq_files/test.source'\n",
    "target_path = '/home/ml/users/cadencao/XSum/fairseq_files/test.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Summarization Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "from utils import tokenize, tokenize_with_mask, generate_sequence, get_probability, get_cmlm_probability, get_prior_probability, cmlm_generate, prior_generate\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 9444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\n",
      "[{'start': 35, 'end': 39, 'label': 'ORG'}, {'start': 75, 'end': 82, 'label': 'ORG'}]\n",
      "\n",
      "- entity: TTTS\n",
      "- prior: 0.00037639751099050045\n",
      "- posterior: 0.7503890991210938\n",
      "- ratio: 0.750 / 0.000 = 1942.013\n",
      "\n",
      "- entity: Cardiff\n",
      "- prior: 0.0011739730834960938\n",
      "- posterior: 0.1402587890625\n",
      "- ratio: 0.140 / 0.001 = 118.465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = xsum_source[INDEX]\n",
    "target = \"Twin-to-twin transfusion syndrome (TTTS) is being tracked by a hospital in Cardiff in a bid to save the lives of babies born with the condition.\"\n",
    "print(target)\n",
    "\n",
    "ent_parts = nlp(target).to_json()['ents']\n",
    "print(ent_parts, end='\\n\\n')\n",
    "\n",
    "for e in ent_parts:\n",
    "    entity = target[e['start']: e['end']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "        prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "#         print(target[0: e['start']] + '<mask>' + target[e['end']:])\n",
    "#         print('- prior: {}'.format(prior_generate(bart, masked_hypothesis)))\n",
    "\n",
    "        masked_hypothesis = target[0: e['start']] + '###' + target[e['end']:]\n",
    "        masked_hypothesis = '<s> ' + masked_hypothesis + ' <\\s> ' + source\n",
    "        posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                         '<s> ' + target,\n",
    "                                         masked_hypothesis,\n",
    "                                         (e['start'] + 4, e['end'] + 4),\n",
    "                                         entity, verbose=False)\n",
    "#         print(target[0: e['start']] + '###' + target[e['end']:])\n",
    "#         print('- posterior: {}'.format(cmlm_generate(finetuned_bart, masked_hypothesis, verbose=False)))\n",
    "\n",
    "        print('- entity: {}'.format(entity))\n",
    "        print('- prior: {}'.format(prior))\n",
    "        print('- posterior: {}'.format(posterior))\n",
    "        print('- ratio: {:.3f} / {:.3f} = {:.3f}'.format(posterior, prior, posterior / (prior + 1e-5)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(data):\n",
    "    for INDEX in tqdm(range(len(data))):\n",
    "        source = xsum_source[data[INDEX]['id']]\n",
    "        avg_posterior = 0.\n",
    "\n",
    "        for i, e in enumerate(data[INDEX]['ents']):\n",
    "            target = data[INDEX]['pred']\n",
    "            entity = e['ent']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    masked_hypothesis = target[0: e['start']] + '<mask>' + target[e['end']:]\n",
    "                    prior = get_prior_probability(bart, target, masked_hypothesis, (e['start'], e['end']), entity)\n",
    "\n",
    "                    masked_hypothesis = '<s> ' + target[0: e['start']] + '###' + target[e['end']:]\n",
    "                    masked_input = masked_hypothesis + ' <\\s> ' + source\n",
    "                    posterior = get_cmlm_probability(finetuned_bart,\n",
    "                                                     '<s> ' + target,\n",
    "                                                     masked_input,\n",
    "                                                     (e['start'] + 4, e['end'] + 4),\n",
    "                                                     entity, verbose=False)\n",
    "\n",
    "                    e['prior'], e['posterior'] = prior, posterior\n",
    "                    avg_posterior += posterior\n",
    "                except:\n",
    "                    print('- Got an error!')\n",
    "\n",
    "        if len(data[INDEX]['ents']) > 0:\n",
    "            data[INDEX]['avg_posterior'] = avg_posterior / len(data[INDEX]['ents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:02, 142.67it/s]\n",
      " 77%|███████▋  | 232/300 [13:02<04:55,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Got an error!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [17:09<00:00,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: 9444\n",
      "0.6241776715763897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_path = 'preds/xsum_binary_smoothly_weighted.hypo'\n",
    "xsum_preds = read_lines(target_path)\n",
    "data = []\n",
    "\n",
    "for i, t in tqdm(enumerate(xsum_preds[:500])):\n",
    "    item = {}\n",
    "    item['id'] = i\n",
    "    item['pred'] = t\n",
    "    item['ents'] = nlp(t).to_json()['ents']\n",
    "\n",
    "    for e in item['ents']:\n",
    "        e['ent'] = item['pred'][e['start']: e['end']]\n",
    "\n",
    "    data.append(item)\n",
    "\n",
    "get_posterior(data)\n",
    "\n",
    "avg_posterior, counter = 0., 0.\n",
    "\n",
    "for d in data:\n",
    "    if 'avg_posterior' in d:\n",
    "        avg_posterior += d['avg_posterior']\n",
    "        counter += 1\n",
    "\n",
    "print('checkpoint: {}'.format(INDEX))\n",
    "print(avg_posterior / counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: 0.5103260373198956\n",
    "# data-dropout: 0.6402941753973592\n",
    "# xsum_official: 0.6002646216892829\n",
    "# xsum self-trained on Cedar: 0.5992162492884309\n",
    "# xsum_binary_smoothly_weighted (200): 0.6287983280432797\n",
    "# xsum_binary_weighted (200): 0.6580150275871669\n",
    "\n",
    "# xsum cedar checkpoint1 (100): 0.6132371826385169\n",
    "# xsum cedar checkpoint2 (100): 0.61654346501808\n",
    "# xsum cedar checkpoint3 (100): 0.6229014194340766\n",
    "\n",
    "# xsum cedar checkpoint1 (500): 0.5925646644582644\n",
    "# xsum cedar checkpoint2 (500): 0.60001381397845\n",
    "# xsum cedar checkpoint3 (500): 0.603983573738802\n",
    "\n",
    "# xsum cedar checkpoint1 (1000): 0.6070493213913469\n",
    "# xsum cedar checkpoint2 (1000): 0.6027837024769428\n",
    "# xsum cedar checkpoint3 (1000): 0.6016024746051729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
